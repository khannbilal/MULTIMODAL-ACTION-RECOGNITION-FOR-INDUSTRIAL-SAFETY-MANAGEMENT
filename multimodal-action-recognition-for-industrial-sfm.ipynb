{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11477745,"sourceType":"datasetVersion","datasetId":7179722},{"sourceId":13497931,"sourceType":"datasetVersion","datasetId":8570166}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Step 0\nimport os\nimport numpy as np\nimport soundfile as sf\n\ndataset_root = \"/kaggle/input/fan-and-valve\"\npreprocessed_root = \"/kaggle/working/preprocessed_augmented/fan-and-valve\"\nos.makedirs(preprocessed_root, exist_ok=True)\n\nclip_duration = 5  # seconds\nstride = 1.25\nsample_rate = 16000\nclasses = [\"fan\", \"valve\"]\n\nfor cls in classes:\n    cls_dir = os.path.join(dataset_root, cls, cls)\n    audio_files = [f for f in os.listdir(cls_dir) if f.endswith(\".wav\")]\n    clip_count = 0\n    for audio_file in audio_files:\n        audio_path = os.path.join(cls_dir, audio_file)\n        audio, sr = sf.read(audio_path)\n        if sr != sample_rate:\n            continue\n        total_samples = len(audio)\n        samples_per_clip = clip_duration * sample_rate\n        stride_samples = int(stride * sample_rate)\n        start = 0\n        while start + samples_per_clip <= total_samples:\n            audio_clip = audio[start:start + samples_per_clip]\n            clip_dir = os.path.join(preprocessed_root, cls, f\"clip_{clip_count:04d}\")\n            os.makedirs(clip_dir, exist_ok=True)\n            np.save(os.path.join(clip_dir, \"audio.npy\"), audio_clip)\n            # Placeholder frames\n            np.save(os.path.join(clip_dir, \"frames.npy\"), np.zeros((75,224,224,3), dtype=np.uint8))\n            clip_count += 1\n            start += stride_samples\n        # Ensure at least 100 clips per class\n        while clip_count < 100:\n            rand_idx = np.random.randint(0, clip_count)\n            existing_clip = os.path.join(preprocessed_root, cls, f\"clip_{rand_idx:04d}\")\n            audio = np.load(os.path.join(existing_clip, \"audio.npy\"))\n            frames = np.load(os.path.join(existing_clip, \"frames.npy\"))\n            clip_dir = os.path.join(preprocessed_root, cls, f\"clip_{clip_count:04d}\")\n            os.makedirs(clip_dir, exist_ok=True)\n            np.save(os.path.join(clip_dir, \"audio.npy\"), audio)\n            np.save(os.path.join(clip_dir, \"frames.npy\"), frames)\n            clip_count += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T10:15:29.777744Z","iopub.execute_input":"2025-10-29T10:15:29.778060Z","iopub.status.idle":"2025-10-29T10:15:29.789222Z","shell.execute_reply.started":"2025-10-29T10:15:29.778009Z","shell.execute_reply":"2025-10-29T10:15:29.788550Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"#Step 1\nimport os\nimport numpy as np\nimport math\nimport pandas as pd\n\nsrc_root = \"/kaggle/working/preprocessed/fan-and-valve\"\ndst_root = \"/kaggle/working/preprocessed_augmented/fan-and-valve\"\nos.makedirs(dst_root, exist_ok=True)\n\nwindow_frames = 75\nstride_frames = 15\naudio_rate = 16000\nwindow_audio = 5 * audio_rate\nstride_audio = 1 * audio_rate\n\nmetadata = []\n\nfor cls in os.listdir(src_root):\n    cls_src = os.path.join(src_root, cls)\n    cls_dst = os.path.join(dst_root, cls)\n    os.makedirs(cls_dst, exist_ok=True)\n    for clip in os.listdir(cls_src):\n        clip_dir = os.path.join(cls_src, clip)\n        frames_path = os.path.join(clip_dir, \"frames.npy\")\n        audio_path = os.path.join(clip_dir, \"audio.npy\")\n        if not os.path.exists(frames_path) or not os.path.exists(audio_path):\n            continue\n        frames = np.load(frames_path)\n        audio = np.load(audio_path)\n        num_aug = math.floor((len(frames) - window_frames)/stride_frames) + 1\n        for i in range(num_aug):\n            f_start, f_end = i*stride_frames, i*stride_frames+window_frames\n            a_start, a_end = i*stride_audio, i*stride_audio+window_audio\n            if f_end > len(frames) or a_end > len(audio):\n                break\n            out_dir = os.path.join(cls_dst, f\"{clip}_aug{i:02d}\")\n            os.makedirs(out_dir, exist_ok=True)\n            np.save(os.path.join(out_dir, \"frames.npy\"), frames[f_start:f_end])\n            np.save(os.path.join(out_dir, \"audio.npy\"), audio[a_start:a_end])\n            metadata.append({\n                \"class\": cls,\n                \"original_clip\": clip,\n                \"augmented_clip\": f\"{clip}_aug{i:02d}\"\n            })\n\nmetadata_df = pd.DataFrame(metadata)\nmetadata_path = \"/kaggle/working/split_metadata.csv\"\nmetadata_df.to_csv(metadata_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T10:15:29.798181Z","iopub.execute_input":"2025-10-29T10:15:29.798379Z","iopub.status.idle":"2025-10-29T10:15:29.896153Z","shell.execute_reply.started":"2025-10-29T10:15:29.798364Z","shell.execute_reply":"2025-10-29T10:15:29.895560Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"#Step 2\nimport os\nimport numpy as np\n\ndataset_root = \"/kaggle/working/preprocessed/fan-and-valve\"\naugmented_root = \"/kaggle/working/preprocessed_augmented/fan-and-valve\"\nos.makedirs(augmented_root, exist_ok=True)\n\nclip_duration = 5.0\nsample_rate = 16000\nstride = 0.25\nsamples_per_clip = int(clip_duration * sample_rate)\nstride_samples = int(stride * sample_rate)\n\nfor class_name in [\"fan\",\"valve\"]:\n    class_dir = os.path.join(dataset_root,class_name)\n    output_dir = os.path.join(augmented_root,class_name)\n    os.makedirs(output_dir,exist_ok=True)\n    clip_counter = 0\n    for clip_folder in sorted(os.listdir(class_dir)):\n        clip_path = os.path.join(class_dir, clip_folder)\n        audio_path = os.path.join(clip_path,\"audio.npy\")\n        frames_path = os.path.join(clip_path,\"frames.npy\")\n        if not os.path.exists(audio_path) or not os.path.exists(frames_path):\n            continue\n        audio = np.load(audio_path)\n        frames = np.load(frames_path)\n        for start in range(0,len(audio)-samples_per_clip+1,stride_samples):\n            end = start + samples_per_clip\n            audio_slice = audio[start:end]\n            out_dir = os.path.join(output_dir,f\"{clip_folder}_aug_{clip_counter:04d}\")\n            os.makedirs(out_dir,exist_ok=True)\n            np.save(os.path.join(out_dir,\"audio.npy\"),audio_slice)\n            np.save(os.path.join(out_dir,\"frames.npy\"),frames)\n            clip_counter += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T10:15:29.897393Z","iopub.execute_input":"2025-10-29T10:15:29.897638Z","iopub.status.idle":"2025-10-29T10:15:29.971380Z","shell.execute_reply.started":"2025-10-29T10:15:29.897614Z","shell.execute_reply":"2025-10-29T10:15:29.970566Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"#Step 3\nimport os\nimport numpy as np\nimport pandas as pd\n\npreprocessed_root = \"/kaggle/working/preprocessed_augmented/fan-and-valve\"\nreport = []\n\nfor class_name in [\"fan\",\"valve\"]:\n    class_dir = os.path.join(preprocessed_root,class_name)\n    if not os.path.exists(class_dir):\n        continue\n    clip_dirs = [os.path.join(class_dir,d) for d in os.listdir(class_dir) if os.path.isdir(os.path.join(class_dir,d))]\n    for clip_dir in clip_dirs:\n        frames_path = os.path.join(clip_dir,\"frames.npy\")\n        audio_path = os.path.join(clip_dir,\"audio.npy\")\n        frames_shape = None\n        audio_shape = None\n        if os.path.exists(frames_path):\n            frames_shape = np.load(frames_path).shape\n        if os.path.exists(audio_path):\n            audio_shape = np.load(audio_path).shape\n        report.append({\n            \"class\": class_name,\n            \"clip\": os.path.basename(clip_dir),\n            \"frames_shape\": frames_shape,\n            \"audio_shape\": audio_shape\n        })\n\ndf = pd.DataFrame(report)\nsummary = df.groupby(\"class\").size().reset_index(name=\"total_clips\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T10:15:29.972181Z","iopub.execute_input":"2025-10-29T10:15:29.972372Z","iopub.status.idle":"2025-10-29T10:15:30.002528Z","shell.execute_reply.started":"2025-10-29T10:15:29.972357Z","shell.execute_reply":"2025-10-29T10:15:30.001986Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"# Step 4: Embedding Verification & Visualization\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\n\npreprocessed_root = \"/kaggle/working/preprocessed_augmented/fan-and-valve\"\nreport = []\nlabels = []\n\nfor cls in [\"fan\", \"valve\"]:\n    cls_dir = os.path.join(preprocessed_root, cls)\n    clip_dirs = [os.path.join(cls_dir, d) for d in os.listdir(cls_dir) if os.path.isdir(os.path.join(cls_dir, d))]\n    for clip_dir in clip_dirs:\n        frames_path = os.path.join(clip_dir, \"frames.npy\")\n        audio_path = os.path.join(clip_dir, \"audio.npy\")\n        if os.path.exists(frames_path) and os.path.exists(audio_path):\n            report.append(np.concatenate([np.load(frames_path).flatten(), np.load(audio_path).flatten()]))\n            labels.append(cls)\n\nX = np.array(report)\ny = LabelEncoder().fit_transform(labels)\n\npca = PCA(n_components=2)\nX_2d = pca.fit_transform(X)\nplt.figure(figsize=(6,6))\nfor label in np.unique(y):\n    plt.scatter(X_2d[y==label,0], X_2d[y==label,1], label=f\"Class {label}\")\nplt.title(\"PCA of Preprocessed Clips\")\nplt.legend()\nplt.show()\n\n#Step 4\nimport os\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models.video import r3d_18\nfrom PIL import Image\nimport torchaudio\n\nbase_dir = \"/kaggle/working/preprocessed_augmented\"\nembed_dir = \"/kaggle/working/embeddings\"\nos.makedirs(embed_dir, exist_ok=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvideo_model = r3d_18(weights=\"KINETICS400_V1\")\nvideo_model.fc = torch.nn.Identity()\nvideo_model = video_model.to(device).eval()\n\naudio_bundle = torchaudio.pipelines.WAV2VEC2_BASE\naudio_model = audio_bundle.get_model().to(device).eval()\n\ntransform = T.Compose([\n    T.Resize((112, 112)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.43216, 0.394666, 0.37645],\n                std=[0.22803, 0.22145, 0.216989]),\n])\n\ndef extract_visual_features(video_frames):\n    frames = torch.stack([transform(Image.fromarray(f.astype(np.uint8))) for f in video_frames])\n    frames = frames.permute(1, 0, 2, 3).unsqueeze(0).float().to(device)\n    with torch.no_grad():\n        feats = video_model(frames)\n    return feats.squeeze(0).cpu().numpy()\n\ndef extract_audio_features(audio_waveform):\n    waveform = torch.tensor(audio_waveform, dtype=torch.float32).unsqueeze(0).to(device)\n    with torch.no_grad():\n        feats, _ = audio_model.extract_features(waveform)\n    return feats[-1].squeeze(0).cpu().numpy()\n\nfor cls in os.listdir(base_dir):\n    cls_path = os.path.join(base_dir, cls)\n    if not os.path.isdir(cls_path):\n        continue\n    for clip in os.listdir(cls_path):\n        clip_path = os.path.join(cls_path, clip)\n        frames_path = os.path.join(clip_path, \"frames.npy\")\n        audio_path = os.path.join(clip_path, \"audio.npy\")\n        try:\n            if os.path.exists(frames_path):\n                frames = np.load(frames_path)\n                vis_feats = extract_visual_features(frames)\n                np.save(os.path.join(embed_dir, f\"{cls}_{clip}_visual.npy\"), vis_feats)\n            if os.path.exists(audio_path):\n                audio = np.load(audio_path)\n                aud_feats = extract_audio_features(audio)\n                np.save(os.path.join(embed_dir, f\"{cls}_{clip}_audio.npy\"), aud_feats)\n        except Exception as e:\n            print(f\"Error in {cls}/{clip}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T10:15:30.004126Z","iopub.execute_input":"2025-10-29T10:15:30.004761Z","iopub.status.idle":"2025-10-29T10:15:45.717217Z","shell.execute_reply.started":"2025-10-29T10:15:30.004736Z","shell.execute_reply":"2025-10-29T10:15:45.716615Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"#Step 5\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nEMB_ROOT = \"/kaggle/working/embeddings\"\nFUSED_ROOT = \"/kaggle/working/embeddings/fused\"\nos.makedirs(FUSED_ROOT, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass CrossAttentionFusion(nn.Module):\n    def __init__(self, v_dim, a_dim, hidden=512, nheads=8, out_dim=512):\n        super().__init__()\n        self.v_proj = nn.Linear(v_dim, hidden)\n        self.a_proj = nn.Linear(a_dim, hidden)\n        self.attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=nheads, batch_first=True)\n        self.ff = nn.Sequential(nn.Linear(hidden * 2, out_dim), nn.ReLU(), nn.Linear(out_dim, out_dim))\n    def forward(self, v, a):\n        if v.ndim == 1: v = v.unsqueeze(0).unsqueeze(1)\n        elif v.ndim == 2: v = v.unsqueeze(0)\n        if a.ndim == 1: a = a.unsqueeze(0).unsqueeze(1)\n        elif a.ndim == 2: a = a.unsqueeze(0)\n        v_h = self.v_proj(v.float())\n        a_h = self.a_proj(a.float())\n        attn_out, _ = self.attn(query=v_h, key=a_h, value=a_h)\n        a_pool = a_h.mean(dim=1, keepdim=True)\n        cat = torch.cat([attn_out, a_pool], dim=-1).squeeze(1)\n        return self.ff(cat)\n\nvisual_files = [f for f in os.listdir(EMB_ROOT) if f.endswith(\"_visual.npy\")]\naudio_files = [f for f in os.listdir(EMB_ROOT) if f.endswith(\"_audio.npy\")]\nclip_ids = set(f.replace(\"_visual.npy\", \"\") for f in visual_files) & set(f.replace(\"_audio.npy\", \"\") for f in audio_files)\n\nif not clip_ids:\n    raise ValueError(\"No matching visual/audio embeddings found!\")\n\nsample_v = np.load(os.path.join(EMB_ROOT, visual_files[0]))\nsample_a = np.load(os.path.join(EMB_ROOT, audio_files[0]))\nv_dim, a_dim = sample_v.shape[-1], sample_a.shape[-1]\nmodel = CrossAttentionFusion(v_dim=v_dim, a_dim=a_dim).to(device).eval()\n\nfor cid in sorted(clip_ids):\n    v = np.load(os.path.join(EMB_ROOT, f\"{cid}_visual.npy\"))\n    a = np.load(os.path.join(EMB_ROOT, f\"{cid}_audio.npy\"))\n    if v.ndim == 1: v = v.reshape(1, -1)\n    if a.ndim == 1: a = a.reshape(1, -1)\n    v_tensor = torch.from_numpy(v).float().to(device)\n    a_tensor = torch.from_numpy(a).float().to(device)\n    with torch.no_grad():\n        fused = model(v_tensor, a_tensor).cpu().numpy()\n    np.save(os.path.join(FUSED_ROOT, f\"{cid}_fused.npy\"), fused)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T10:15:45.717893Z","iopub.execute_input":"2025-10-29T10:15:45.718107Z","iopub.status.idle":"2025-10-29T10:15:45.804823Z","shell.execute_reply.started":"2025-10-29T10:15:45.718092Z","shell.execute_reply":"2025-10-29T10:15:45.804296Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"import os, json, numpy as np, torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nemb_dir = \"/kaggle/working/embeddings/fused\"\nresults_dir = \"/kaggle/working/results\"\nos.makedirs(results_dir, exist_ok=True)\n\nX, y = [], []\nfor f in os.listdir(emb_dir):\n    if f.endswith(\".npy\"):\n        arr = np.load(os.path.join(emb_dir, f))\n        arr = arr.flatten() if arr.ndim > 1 else arr\n        X.append(arr)\n        label = f.split(\"_\")[1] if \"_\" in f else \"unknown\"\n        y.append(label)\n\nX, y = np.array(X), np.array(y)\nle = LabelEncoder()\ny_enc = le.fit_transform(y)\nX_tensor = torch.tensor(X, dtype=torch.float32)\nnum_classes = len(np.unique(y_enc))\ninput_dim = X_tensor.shape[1]\n\nclass Classifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.relu = nn.ReLU()\n        self.drop = nn.Dropout(0.4)\n        self.fc2 = nn.Linear(128, num_classes)\n        self.softmax = nn.Softmax(dim=1)\n    def forward(self, x):\n        if x.ndim == 1: x = x.unsqueeze(0)\n        return self.softmax(self.fc2(self.drop(self.relu(self.fc1(x)))))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nall_acc, all_prec, all_rec, all_f1 = [], [], [], []\n\nfor train_idx, test_idx in skf.split(X_tensor, y_enc):\n    X_train, X_test = X_tensor[train_idx].to(device), X_tensor[test_idx].to(device)\n    y_train, y_test = torch.tensor(y_enc[train_idx]).to(device), torch.tensor(y_enc[test_idx]).to(device)\n\n    model = Classifier(input_dim, num_classes).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n\n    for epoch in range(25):\n        model.train()\n        optimizer.zero_grad()\n        outputs = model(X_train)\n        loss = criterion(outputs, y_train)\n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        preds = model(X_test).argmax(1).cpu().numpy()\n    y_test_cpu = y_test.cpu().numpy()\n    acc = accuracy_score(y_test_cpu, preds)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_test_cpu, preds, average=\"weighted\", zero_division=0)\n    all_acc.append(acc)\n    all_prec.append(prec)\n    all_rec.append(rec)\n    all_f1.append(f1)\n\nmetrics = {\n    \"accuracy\": float(np.mean(all_acc)),\n    \"precision\": float(np.mean(all_prec)),\n    \"recall\": float(np.mean(all_rec)),\n    \"f1_score\": float(np.mean(all_f1))\n}\n\nwith open(os.path.join(results_dir, \"metrics.json\"), \"w\") as f:\n    json.dump(metrics, f, indent=4)\n\nprint(metrics)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}